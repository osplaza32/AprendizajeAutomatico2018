{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<img src='figuras/Agente.png' width=\"75%\"/>\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje de refuerzo (RL) es aprender qué hacer, dada una situación y un conjunto de posibles acciones para elegir, con el fin de maximizar una recompensa. Al alumno, que llamaremos agente, no se le dice qué hacer, debe descubrirlo por sí mismo a través de la interacción con el entorno. El objetivo es elegir sus acciones de tal manera que la recompensa acumulada se maximice. Entonces, elegir la mejor recompensa ahora, podría no ser la mejor decisión, a la larga. Esos enfoques codiciosos podrían no ser óptimos.\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<img src='figuras/TiposDeAprendizaje.png' width=\"75%\"/>\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje por refuerzo es diferente del aprendizaje supervisado, el tipo de aprendizaje estudiado\n",
    "en la mayoría de las aplicaciones en el campo del aprendizaje automático. El aprendizaje supervisado es\n",
    "aprender de un conjunto de entrenamiento de ejemplos etiquetados proporcionados por un supervisor externo.\n",
    "Cada ejemplo es una descripción de una situación junto con una especificación, la etiqueta, de la acción correcta que el sistema debería tomar en esa situación, que a menudo es identificar la categoría a la que pertenece la situación El objetivo de este tipo de aprendizaje es que el sistema extrapole o generalice sus respuestas para que actúe correctamente en situaciones que no están presentes en el conjunto de entrenamiento. Este es un tipo importante de aprendizaje, pero por si solo no es adecuado para aprender de la interacción. En problemas interactivos, a menudo no es práctico obtener ejemplos del comportamiento deseados que sean correctos y representativos de todas las situaciones en las que el agente tiene que actuar. En territorio inexplorado, donde uno esperaría que el aprendizaje sea más beneficioso, un agente debe ser capaz de aprender de su propia experiencia.\n",
    "\n",
    "El aprendizaje por refuerzo también es diferente del aprendizaje no supervisado, que generalmente trata de encontrar la estructura oculta en colecciones de datos sin etiqueta. Los términos aprendizaje supervisado y aprendizaje no supervisado parecerían clasificar exhaustivamente los paradigmas de aprendizaje automático, pero no es así. Aunque uno podría estar tentado a pensar en el aprendizaje por refuerzo como una especie de aprendizaje no supervisado porque no se basa en ejemplos de comportamiento correcto, el aprendizaje por refuerzo está intentando maximizar una señal de recompensa en lugar de tratar de encontrar la estructura oculta. Descubriendo la estructura en la experiencia de un agente ciertamente puede ser útil en el aprendizaje por refuerzo, pero por sí mismo no aborda el problema de aprendizaje por resfuerzo de maximizar la señal de recompensa.\n",
    "\n",
    "Por lo tanto, se puede considerar que el aprendizaje por refuerzo es un tercer paradigma de aprendizaje automático, junto con el aprendizaje supervisado y el aprendizaje no supervisado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Balance Carro-Poste\n",
    "\n",
    "<img src='figuras/Car_Pole_Balancing.png' width='50%' />\n",
    "\n",
    "- **Objetivo**: equilibrar el poste sobre un carro en movimiento\n",
    "- **Estado**: ángulo, velocidad angular, posición, velocidad horizontal\n",
    "- **Acciones**: fuerza horizontal al carro\n",
    "- **Recompensa**: 1 en cada paso de tiempo si el poste está en posición vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Juegos de Atari\n",
    "\n",
    "#### Breakout\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"figuras/atari.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje más alto\n",
    "- **Estado**: Píxeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuación proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pac-Mac\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"figuras/PacMac.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "https://github.com/tychovdo/PacmanDQN\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje más alto\n",
    "- **Estado**: Píxeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuación proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenar Robots para el embalaje \n",
    "\n",
    "<img src='figuras/Robot.png' width=\"50%\" />\n",
    "\n",
    "- **Objetivo**: Elegir un dispositivo de una caja y ponerlo en un contenedor\n",
    "- **Estado**: Píxeles brutos del mundo real\n",
    "- **Acciones**: Posibles acciones del robot\n",
    "- **Recompensa**: Positiva al colocar un dispositivo con éxito; de lo contrario, negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizando el problema del Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Proceso de Decisión de Markov (MDP por sus siglas en inglés) es una formulación matemática del problema del Aprendizaje por Refuerzo. Que satisfacen la propiedad de Markov:\n",
    "\n",
    "**Propiedad de Markov**: el estado actual representa por completo el estado del ambiente (mundo). Es decir, el futuro depende solo del presente.\n",
    "\n",
    "Un MDP puede definirse por (S, A, R, P, γ) donde:\n",
    "\n",
    "- **S**: conjunto de posibles estados\n",
    "- **A**: conjunto de posibles acciones\n",
    "- **R**: distribución de la probabilidad de la recompensa dado el par (estado, acción)\n",
    "- **P**: distribución de probabilidad de que tan posible que alguno de los estados sea el nuevo estado, dado el par (estado, acción). También conocido como probabilidad de transición.\n",
    "- **γ**: factor de descuento de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo funcionan los MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paso de tiempo inicial $t = 0$, el entorno elige el estado inicial $s_o\\: con\\: p(s_o)$. Ese estado se usa como un estado semilla para el siguiente ciclo:\n",
    "\n",
    "desde $t = 0$ hasta que finalice:\n",
    "\n",
    "- El agente selecciona la acción $a_t$\n",
    "- El entorno elige la recompensa $r_t$ con $R(.|S_t,a_t)$ y el siguiente estado $s_{t + 1}$ con $P(. |S_t, a_t)$\n",
    "- El agente recibe la recompensa $r_t$ y el siguiente estado $s_{t + 1}$\n",
    "\n",
    "Más formalmente, el entorno no elige, toma muestras de las distribuciones de probabilidad de recompensa y transición.\n",
    "\n",
    "¿Cuál es el objetivo de todo esto? Encuentre una función $π *$, conocida como política óptima, que maximiza la recompensa acumulada descontada:\n",
    "\n",
    "$$\\sum_{t\\ge0}\\gamma^tr_t$$\n",
    "\n",
    "Una política $π$ es una función que maoea el estado $s$ a la acción $a$, que nuestro agente cree que es la mejor dado ese estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolviendo un MDP con Q-Learning\n",
    "\n",
    "Es hora de aprender sobre las funciones de valor, la ecuación de Bellman y Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descuento en Recompensas Futuras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué necesitamos el factor de descuento $γ$? La recompensa total que recibirá su agente desde el paso de tiempo actual $t$ hasta el final de la tarea se puede definir como:\n",
    "\n",
    "$$R_t = r_t + r_{t+1} + \\dots + r_n$$\n",
    "\n",
    "Eso se ve bien, pero no olvidemos que nuestro entorno es estocástico. El factor de descuento nos permite valorar la recompensa a corto plazo más que a largo plazo, podemos usarla como:\n",
    "\n",
    "$$R_t = r_t + {\\gamma}r_{t+1} + \\dots + \\gamma^{n-t}r_n = r_t+{\\gamma}R_{t+1}$$\n",
    "\n",
    "Nuestro agente funcionaría de maravilla si elige la acción que maximiza la recompensa futura (descontada) en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sería muy bueno saber cuán \"bueno\" es un estado dado. Algo que nos asegure: no importa el estado en el que se encuentre si realiza la transición al estado $s$ su recompensa total será $x$. Si comienzas desde $s$ y sigues la política $π$. Eso nos evitaría volver a visitar los mismos estados una y otra vez. La **función de valor** lo hace por nosotros. Depende del estado $s$ en que nos encontremos y de la política $\\pi$ que su agente esté siguiendo. Está dada por:\n",
    "\n",
    "$$V^\\pi(s)=\\mathbb{E}(\\sum_{t\\ge0}\\gamma^tr_t)\\:\\:\\:{\\forall}s\\in\\mathbb{S}$$\n",
    "\n",
    "Existe una función de valor óptimo que tiene el valor más alto para todos los estados. Está dada por:\n",
    "\n",
    "$$V^*(s)=\\max_{\\pi}V^\\pi(s)\\:\\:\\:{\\forall}s\\in\\mathbb{S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, su agente no puede controlar en qué estado termina directamente. Él puede influenciarlo eligiendo alguna acción $a$. Introduzquemos otra función que acepte el estado y la acción como parámetros y devuelva la recompensa total esperada: la función $Q$ (representa la \"calidad\" de una determinada acción dado un estado). Más formalmente, la función $Q^π(s, a)$ da el retorno esperado al comenzar en $s$, realizar $a$ y seguir a $π$.\n",
    "\n",
    "De nuevo, podemos definir la función $Q$ óptima $Q^*(s, a)$ que da la recompensa total esperada para su agente cuando comienza en $s$ y elige la acción $a$. Es decir, la función $Q$ óptima le dice a su agente qué tan buena es elegir una acción $a$ cuando está en el estado $s$.\n",
    "\n",
    "Existe una relación entre las dos funciones óptimas $V^*$ y $Q^*$. Está dada por:\n",
    "\n",
    "$$V^*(s)=\\max_{a}Q^*(s,a)\\:\\:\\:{\\forall}s\\in\\mathbb{S}$$\n",
    "\n",
    "Es decir, la recompensa total máxima esperada cuando se inicia en $s$ es el máximo de $Q^*(s, a)$ sobre todas las acciones posibles.\n",
    "\n",
    "Usando $Q^*(s, a)$ podemos extraer la política óptima $π^*$ eligiendo la acción $a$ que da la máxima recompensa $Q^*(s, a)$ para el estado $s$. Tenemos:\n",
    "\n",
    "$$\\pi^*(s)=arg\\max_aQ^*(s,a)\\:\\:\\:{\\forall}s\\in\\mathbb{S}$$\n",
    "\n",
    "Existe una relación entre todas las funciones que definimos hasta ahora. Ahora tenemos las herramientas para identificar los estados y los pares de acciones y estados como buenos o malos. Más importante aún, si podemos identificar $V^*$ o $Q^*$, podemos crear el mejor agente posible (para el entorno actual). ¿Pero cómo usamos esto en la práctica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje con Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrémonos en un solo estado $s$ y acción $a$. Podemos expresar $Q(s, a)$ recursivamente, en términos del valor $Q$ del siguiente estado $s'$:\n",
    "\n",
    "$$Q(s,a)=r+\\gamma\\max_{a'}Q(s',a')$$\n",
    "\n",
    "Esta ecuación, conocida como la ecuación de Bellman, nos dice que la recompensa futura máxima es la recompensa que el agente recibió por ingresar al estado actual $s$ más la recompensa futura máxima por el próximo estado $s'$. La esencia del Q-learning es que podemos aproximar iterativamente $Q^*$ usando la ecuación de Bellman descrita anteriormente. La ecuación Q-learning está dada por:\n",
    "\n",
    "$$Q_{t+1}(s_t,a_t)=Q_t(s_t,a_t)+\\alpha(r_{t+1}+\\gamma\\max_aQ_t(s_{t+1},a)-Q_t(s_t,a_t))$$\n",
    "\n",
    "donde $α$ es la tasa de aprendizaje que controla cuánto se considera la diferencia entre el valor Q anterior y el nuevo.\n",
    "\n",
    "¿Puede su agente aprender algo usando esto? Al principio, no, las aproximaciones iniciales probablemente serán completamente aleatorias / incorrectas. Sin embargo, a medida que el agente explore cada vez más el entorno, los valores aproximados de $Q$ comenzarán a converger a $Q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu objetivo es llegar al helado, sin chocar con un zombie\n",
    "\n",
    "<img src=\"figuras/Ambiente.png\" width=\"25%\" />\n",
    "\n",
    "Chocar contra un zombie da una recompensa de -100 puntos, tomar una acción es -1 punto de recompenza y comer el helado te da los 1000 puntos de recompenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el ambiente\n",
    "\n",
    "El estado inicial se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente2.png\" width=\"25%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h * *\n",
      "* z *\n",
      "z c *\n"
     ]
    }
   ],
   "source": [
    "ZOMBIE = \"z\"\n",
    "CARRO = \"c\"\n",
    "HELADO = \"h\"\n",
    "VACIO = \"*\"\n",
    "\n",
    "grid = [\n",
    "    [HELADO, VACIO, VACIO],\n",
    "    [VACIO, ZOMBIE, VACIO],\n",
    "    [ZOMBIE, CARRO, VACIO]\n",
    "]\n",
    "\n",
    "for row in grid:\n",
    "    print(' '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos nuestro estado del entorno como una clase que contenga la cuadrícula actual y la posición del carro. Tener un acceso constante a la posición del carro en cada paso nos ayudará a simplificar nuestro código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estado:\n",
    "    \n",
    "    def __init__(self, grid, pos_carro):\n",
    "        self.grid = grid\n",
    "        self.pos_carro = pos_carro\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, Estado) and self.grid == other.grid and self.pos_carro == other.pos_carro\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self.grid) + str(self.pos_carro))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Estado(grid={self.grid}, pos_carro={self.pos_carro})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las acciones posibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARRIBA = 0\n",
    "ABAJO = 1\n",
    "IZQUIERDA = 2\n",
    "DERECHA = 3\n",
    "\n",
    "ACCIONES = [ARRIBA, ABAJO, IZQUIERDA, DERECHA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y el estado inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "estado_inicial = Estado(grid=grid, pos_carro=[2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su agente necesita una forma de interactuar con el entorno, es decir, elegir acciones. Definamos una función que toma el estado actual y una acción y devuelve un nuevo estado, la recompensa y si el episodio se ha completado o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def actuar(estado, accion):\n",
    "    \n",
    "    def nueva_pos_carro(estado, accion):\n",
    "        p = deepcopy(estado.pos_carro)\n",
    "        if accion == ARRIBA:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif accion == ABAJO:\n",
    "            p[0] = min(len(estado.grid) - 1, p[0] + 1)\n",
    "        elif accion == IZQUIERDA:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif accion == DERECHA:\n",
    "            p[1] = min(len(estado.grid[0]) - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Acción desconocida {accion}\")\n",
    "        return p\n",
    "            \n",
    "    p = nueva_pos_carro(estado, accion)\n",
    "    grid_item = estado.grid[p[0]][p[1]]\n",
    "    \n",
    "    nuevo_grid = deepcopy(estado.grid)\n",
    "    \n",
    "    if grid_item == ZOMBIE:\n",
    "        recompenza = -100\n",
    "        final = True\n",
    "        nuevo_grid[p[0]][p[1]] += CARRO\n",
    "    elif grid_item == HELADO:\n",
    "        recompenza = 1000\n",
    "        final = True\n",
    "        nuevo_grid[p[0]][p[1]] += CARRO\n",
    "    elif grid_item == VACIO:\n",
    "        recompenza = -1\n",
    "        final = False\n",
    "        anterior = estado.pos_carro\n",
    "        nuevo_grid[anterior[0]][anterior[1]] = VACIO\n",
    "        nuevo_grid[p[0]][p[1]] = CARRO\n",
    "    elif grid_item == CARRO:\n",
    "        recompenza = -1\n",
    "        final = False\n",
    "    else:\n",
    "        raise ValueError(f\"Elemento del grid desconocido {grid_item}\")\n",
    "    \n",
    "    return Estado(grid=nuevo_grid, pos_carro=p), recompenza, final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro caso, un episodio es comenzar desde el estado inicial y chocar con un Zombie o comer el helado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendiendo a conducir\n",
    "\n",
    "Bien, es hora de implementar el algoritmo Q-learning y llegar al helado. Tenemos un espacio de estado muy pequeño, solo 12 estados. Esto nos permite mantener las cosas simples y almacenar los valores $Q$ calculados en una tabla. Comencemos con algunas constantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(42) # para reproducibilidad\n",
    "\n",
    "NUM_ESTADOS = 12\n",
    "NUM_EPISODIOS = 100\n",
    "\n",
    "MAX_PASOS_EPISODIO = 100\n",
    "\n",
    "MIN_ALFA = 0.02\n",
    "\n",
    "alfas = np.linspace(1.0, MIN_ALFA, NUM_EPISODIOS)\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "\n",
    "q_tabla = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bajaremos la tasa de aprendizaje, alfa, por cada episodio: a medida que su agente explore cada vez más el entorno, \"creerá\" que no queda mucho por aprender. Además, se definen límites para la cantidad de episodios y pasos de entrenamiento.\n",
    "\n",
    "Crearemos una función auxiliar `q` que devuelve el valor $Q$ para un par de acción - estado o  todas las acciones, dado un estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(estado, accion=None):\n",
    "    \n",
    "    if estado not in q_tabla:\n",
    "        q_tabla[estado] = np.zeros(len(ACCIONES))\n",
    "        \n",
    "    if accion is None:\n",
    "        return q_tabla[estado]\n",
    "    \n",
    "    return q_tabla[estado][accion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegir una acción dado el estado actual es realmente simple: actuar con una acción aleatoria con una probabilidad pequeña o la mejor acción observada hasta el momento (usando nuestra q_tabla):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccionar_accion(estado):\n",
    "    if random.uniform(0, 1) < eps:\n",
    "        return random.choice(ACCIONES) \n",
    "    else:\n",
    "        return np.argmax(q(estado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué el agente realiza algunas veces acciones aleatorias? Recuerde, el entorno es desconocido, por lo que debe ser explorado de alguna manera; su agente lo hará utilizando el poder de la aleatoriedad.\n",
    "A continuación, entrenando a su agente usando el algoritmo Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1: recompenza total -> -100\n",
      "Episodio 2: recompenza total -> -100\n",
      "Episodio 3: recompenza total -> -101\n",
      "Episodio 4: recompenza total -> -112\n",
      "Episodio 5: recompenza total -> -113\n",
      "Episodio 6: recompenza total -> 989\n",
      "Episodio 7: recompenza total -> -100\n",
      "Episodio 8: recompenza total -> 990\n",
      "Episodio 9: recompenza total -> 994\n",
      "Episodio 10: recompenza total -> 993\n",
      "Episodio 11: recompenza total -> 994\n",
      "Episodio 12: recompenza total -> 994\n",
      "Episodio 13: recompenza total -> 996\n",
      "Episodio 14: recompenza total -> 996\n",
      "Episodio 15: recompenza total -> 996\n",
      "Episodio 16: recompenza total -> 994\n",
      "Episodio 17: recompenza total -> 992\n",
      "Episodio 18: recompenza total -> 994\n",
      "Episodio 19: recompenza total -> 996\n",
      "Episodio 20: recompenza total -> 995\n",
      "Episodio 21: recompenza total -> 996\n",
      "Episodio 22: recompenza total -> 995\n",
      "Episodio 23: recompenza total -> 994\n",
      "Episodio 24: recompenza total -> -102\n",
      "Episodio 25: recompenza total -> 993\n",
      "Episodio 26: recompenza total -> 996\n",
      "Episodio 27: recompenza total -> 994\n",
      "Episodio 28: recompenza total -> 996\n",
      "Episodio 29: recompenza total -> 996\n",
      "Episodio 30: recompenza total -> -100\n",
      "Episodio 31: recompenza total -> 993\n",
      "Episodio 32: recompenza total -> 995\n",
      "Episodio 33: recompenza total -> 996\n",
      "Episodio 34: recompenza total -> 996\n",
      "Episodio 35: recompenza total -> 994\n",
      "Episodio 36: recompenza total -> -102\n",
      "Episodio 37: recompenza total -> 996\n",
      "Episodio 38: recompenza total -> 995\n",
      "Episodio 39: recompenza total -> 996\n",
      "Episodio 40: recompenza total -> 995\n",
      "Episodio 41: recompenza total -> 996\n",
      "Episodio 42: recompenza total -> -100\n",
      "Episodio 43: recompenza total -> 996\n",
      "Episodio 44: recompenza total -> -101\n",
      "Episodio 45: recompenza total -> 996\n",
      "Episodio 46: recompenza total -> 995\n",
      "Episodio 47: recompenza total -> 993\n",
      "Episodio 48: recompenza total -> -100\n",
      "Episodio 49: recompenza total -> 995\n",
      "Episodio 50: recompenza total -> 994\n",
      "Episodio 51: recompenza total -> 996\n",
      "Episodio 52: recompenza total -> -100\n",
      "Episodio 53: recompenza total -> 996\n",
      "Episodio 54: recompenza total -> -105\n",
      "Episodio 55: recompenza total -> -102\n",
      "Episodio 56: recompenza total -> 996\n",
      "Episodio 57: recompenza total -> 996\n",
      "Episodio 58: recompenza total -> 994\n",
      "Episodio 59: recompenza total -> 996\n",
      "Episodio 60: recompenza total -> -104\n",
      "Episodio 61: recompenza total -> 995\n",
      "Episodio 62: recompenza total -> 996\n",
      "Episodio 63: recompenza total -> 996\n",
      "Episodio 64: recompenza total -> 996\n",
      "Episodio 65: recompenza total -> 994\n",
      "Episodio 66: recompenza total -> 996\n",
      "Episodio 67: recompenza total -> 996\n",
      "Episodio 68: recompenza total -> 996\n",
      "Episodio 69: recompenza total -> -102\n",
      "Episodio 70: recompenza total -> 994\n",
      "Episodio 71: recompenza total -> 995\n",
      "Episodio 72: recompenza total -> 995\n",
      "Episodio 73: recompenza total -> 996\n",
      "Episodio 74: recompenza total -> 996\n",
      "Episodio 75: recompenza total -> 996\n",
      "Episodio 76: recompenza total -> 996\n",
      "Episodio 77: recompenza total -> 993\n",
      "Episodio 78: recompenza total -> 995\n",
      "Episodio 79: recompenza total -> 995\n",
      "Episodio 80: recompenza total -> 994\n",
      "Episodio 81: recompenza total -> 995\n",
      "Episodio 82: recompenza total -> -104\n",
      "Episodio 83: recompenza total -> -100\n",
      "Episodio 84: recompenza total -> 996\n",
      "Episodio 85: recompenza total -> 996\n",
      "Episodio 86: recompenza total -> -100\n",
      "Episodio 87: recompenza total -> 996\n",
      "Episodio 88: recompenza total -> 996\n",
      "Episodio 89: recompenza total -> 996\n",
      "Episodio 90: recompenza total -> 996\n",
      "Episodio 91: recompenza total -> -100\n",
      "Episodio 92: recompenza total -> 995\n",
      "Episodio 93: recompenza total -> 994\n",
      "Episodio 94: recompenza total -> 994\n",
      "Episodio 95: recompenza total -> 996\n",
      "Episodio 96: recompenza total -> 996\n",
      "Episodio 97: recompenza total -> 994\n",
      "Episodio 98: recompenza total -> 996\n",
      "Episodio 99: recompenza total -> 995\n",
      "Episodio 100: recompenza total -> 995\n"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EPISODIOS):\n",
    "    \n",
    "    estado = estado_inicial\n",
    "    recompenza_total = 0\n",
    "    alfa = alfas[e]\n",
    "    \n",
    "    for _ in range(MAX_PASOS_EPISODIO):\n",
    "        accion = seleccionar_accion(estado)\n",
    "        proximo_estado, recompenza, final = actuar(estado, accion)\n",
    "        recompenza_total += recompenza\n",
    "        \n",
    "        q(estado)[accion] = q(estado, accion) + \\\n",
    "                alfa * (recompenza + gamma *  np.max(q(proximo_estado)) - q(estado, accion))\n",
    "        estado = proximo_estado\n",
    "        if final:\n",
    "            break\n",
    "    print(f\"Episodio {e + 1}: recompenza total -> {recompenza_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos con el estado inicial, en cada episodio, elegimos una acción, recibimos una recompensa y actualizamos nuestros valores $Q$. Tenga en cuenta que la implementación es similar a la fórmula para Q-learning, discutida anteriormente.\n",
    "Puede observar claramente que el agente aprende cómo actuar de manera eficiente, muy rápidamente. Nuestro MDP es realmente pequeño y esto podría ser solo una casualidad. Por otra parte, mirando algunos episodios. puedes ver que el agente choca a un Zombie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Aprendió algo?\n",
    "Extraigamos la política que su agente ha aprendido seleccionando la acción con el valor de Q máximo en cada paso, lo haremos manualmente. Primero, el 'estado_inicial`:\n",
    "\n",
    "<img src=\"figuras/Ambiente2.png\" width=\"25%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=-100.0, down=992.5511459897854, left=-99.99231221308635, right=996.0\n"
     ]
    }
   ],
   "source": [
    "r = q(estado_inicial)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(estado_inicial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DERECHA parece tener el mayor valor de $Q$, tomemos esa acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_estado, recompenza, final = actuar(estado_inicial, DERECHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo estado se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente3.png\" width=\"25%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=997.0, down=978.6832442711817, left=964.4443281827528, right=995.6651916060135\n",
      "Estado(grid=[['h', '*', '*'], ['*', 'z', '*'], ['z', '*', 'c']], pos_carro=[2, 2])\n"
     ]
    }
   ],
   "source": [
    "r = q(nuevo_estado)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(nuevo_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARRIBA parece tener el mayor valor de $Q$, tomemos esa acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Elemento del grid desconocido hc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-38badd647d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnuevo_estado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecompenza\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactuar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnuevo_estado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARRIBA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-5f6772be01f3>\u001b[0m in \u001b[0;36mactuar\u001b[0;34m(estado, accion)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Elemento del grid desconocido {grid_item}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEstado\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnuevo_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_carro\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecompenza\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Elemento del grid desconocido hc"
     ]
    }
   ],
   "source": [
    "nuevo_estado, recompenza, final = actuar(nuevo_estado, ARRIBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo estado se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente4.png\" width=\"25%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=998.0, down=990.4752682114936, left=-99.9557777369302, right=989.6979988897714\n",
      "Estado(grid=[['h', '*', '*'], ['*', 'z', 'c'], ['z', '*', '*']], pos_carro=[1, 2])\n"
     ]
    }
   ],
   "source": [
    "r = q(nuevo_estado)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(nuevo_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARRIBA parece tener el mayor valor de $Q$, tomemos esa acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_estado, recompenza, final = actuar(nuevo_estado, ARRIBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo estado se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente5.png\" width=\"25%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=991.4593174540315, down=785.4157552437505, left=999.0, right=932.8223133238967\n",
      "Estado(grid=[['h', '*', 'c'], ['*', 'z', '*'], ['z', '*', '*']], pos_carro=[0, 2])\n"
     ]
    }
   ],
   "source": [
    "r = q(nuevo_estado)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(nuevo_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IZQUIERDA parece tener el mayor valor de $Q$, tomemos esa acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_estado, recompenza, final = actuar(nuevo_estado, IZQUIERDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo estado se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente6.png\" width=\"25%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=994.3728663382278, down=-98.14574411887469, left=1000.0, right=791.4590276093644\n",
      "Estado(grid=[['h', 'c', '*'], ['*', 'z', '*'], ['z', '*', '*']], pos_carro=[0, 1])\n"
     ]
    }
   ],
   "source": [
    "r = q(nuevo_estado)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(nuevo_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IZQUIERDA parece tener el mayor valor de $Q$, tomemos esa acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_estado, recompenza, final = actuar(nuevo_estado, IZQUIERDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=0.0, down=0.0, left=0.0, right=0.0\n",
      "Estado(grid=[['hc', 'c', '*'], ['*', 'z', '*'], ['z', '*', '*']], pos_carro=[0, 0])\n"
     ]
    }
   ],
   "source": [
    "r = q(nuevo_estado)\n",
    "print(f\"arriba={r[ARRIBA]}, abajo={r[ABAJO]}, izquierda={r[IZQUIERDA]}, derecha={r[DERECHA]}\")\n",
    "print(nuevo_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nuevo estado se ve así:\n",
    "\n",
    "<img src=\"figuras/Ambiente7.png\" width=\"25%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
